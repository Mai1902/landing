{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOPkZ6jo+PAqDmr+9SJJkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mai1902/landing/blob/main/skills_cluster_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install nltk\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "QPvnSRsobVXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KrujTxFZaRnD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from sklearn import feature_extraction\n",
        "from nltk.stem import PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load job description file\n",
        "df = pd.read_csv('/dice_com-job_us_sample.csv')\n",
        "im_df = pd.DataFrame(df, columns = ['company', 'employment', 'jobdescription', 'jobtitle', 'skills'])\n",
        "data_dict = im_df.to_dict()\n",
        "jd_content = [x for x in data_dict['jobdescription'].values()]\n",
        "skills = [x for x in data_dict['skills'].values()]\n",
        "skills_cleaned = []\n",
        "for skill in skills:\n",
        "  skill = str(skill)\n",
        "  skill = skill.replace(',', '')\n",
        "  skills_cleaned.append(skill)\n",
        "\n",
        "jd_content_sample = jd_content[:10000]\n"
      ],
      "metadata": {
        "id": "qvY8G9_DipPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "E50i1sFseU6e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize stop words and stemmer for text processing\n",
        "stopWords = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "bW_dT8bxddYx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epa-sCVWpaoi",
        "outputId": "1e40080f-68e8-4435-ce59-d5e9ee22cfdf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Method to tokenize and stemmized the text'''\n",
        "def tokenize_and_stemmized(text):\n",
        "  tokens = []\n",
        "\n",
        "  #Add all tokenized word into list of token\n",
        "  for sent in nltk.sent_tokenize(text):\n",
        "    for word in nltk.word_tokenize(sent):\n",
        "      tokens.append(word)\n",
        "  filtered_tokens = []\n",
        "\n",
        "  # Filter out token that only contain letter, + sign,underscore, and number\n",
        "  for token in tokens:\n",
        "    if re.search(\"^[A-Za-z0-9+_-]*$\", token):\n",
        "      filtered_tokens.append(token)\n",
        "  stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "  return stems, filtered_tokens\n",
        "\n",
        "stemmed_and_filtered = []\n",
        "filtered_only = []\n",
        "for jd in jd_content:\n",
        "  stemmed_and_filtered = tokenize_and_stemmized(jd)[0]\n",
        "  filtered_only = tokenize_and_stemmized(jd)[1]\n"
      ],
      "metadata": {
        "id": "Y-AWoeGMfHkj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_only[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVRXGuZD3Sz7",
        "outputId": "363ed38e-2e02-47a3-bb70-f31c9760bea7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Experience', 'in', 'ProgrammingDevelopment', 'experience', 'in', 'Win32', 'Programming', 'on', 'Win', 'must', 'have', 'Experience', 'using', 'debuggers', 'such', 'as', 'WinDbgExperience', 'on', 'Windows', 'kernel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "HErpR1friNb-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define vectorizer parameters\n",
        "jd_forfit = [[jd for jd in jd_content_sample]]\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stemmized, ngram_range=(1,3))\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(jd_forfit)\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "NzAViE_Nn0yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szaXjT350Mov",
        "outputId": "cf83e4a1-faa0-43f8-824a-8ff1a9f5d4b3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to LDA approach\n",
        "from gensim import corpora, models, similarities\n"
      ],
      "metadata": {
        "id": "DC4AP9wqsl1U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rake_nltk"
      ],
      "metadata": {
        "id": "PyqKHWTg-oWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rake_nltk import Rake"
      ],
      "metadata": {
        "id": "I22PBfmu-jg0"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get keyword only from jd_content using Rake\n",
        "rake = Rake()\n",
        "def get_kw_rake(jd_content):\n",
        "  for jd in jd_content:\n",
        "    rake.extract_keywords_from_text(jd)\n",
        "  keywords = rake.get_ranked_phrases()\n",
        "  return keywords\n"
      ],
      "metadata": {
        "id": "5JwX5fXg-ui4"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keybert"
      ],
      "metadata": {
        "id": "UIv4UCg3JOTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "bGM1RNgQKJmF"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get keyword only from jd_content using keyBert\n",
        "bert = KeyBERT()\n",
        "def get_kw_bert(jd_content):\n",
        "    for jd in jd_content:\n",
        "      keywords = bert.extract_keywords(jd, keyphrase_ngram_range=(3, 5), stop_words=\"english\", top_n=20)\n",
        "    results = []\n",
        "    for scored_keywords in keywords:\n",
        "        for keyword in scored_keywords:\n",
        "              results.append(keyword)\n",
        "    return results "
      ],
      "metadata": {
        "id": "b8-1_y3UJGlW"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/LIAAD/yake"
      ],
      "metadata": {
        "id": "RXK5JtciWRe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake"
      ],
      "metadata": {
        "id": "a9OzH3uwWg9J"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get keyword only from jd_content using YAKE\n",
        "def get_kw_yake(jd_content):\n",
        "    for jd in jd_content:\n",
        "      keywords = yake.KeywordExtractor(lan=\"en\", n=3, windowsSize=3, top=20).extract_keywords(jd)\n",
        "    results = []\n",
        "    for scored_keywords in keywords:\n",
        "        for keyword in scored_keywords:\n",
        "            if isinstance(keyword, str):\n",
        "                results.append(keyword) \n",
        "    return results "
      ],
      "metadata": {
        "id": "qar-RId4WLUQ"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keywords = get_kw_rake(jd_content[:1000])\n",
        "# remove stop words from list of tokenized and stemmed token\n",
        "#filtered_only = [[word for word in tokenize_and_stemmized(keys)[1]] for keys in keywords]\n",
        "filtered_only = [[word for word in tokenize_and_stemmized(skills)[1]] for skills in skills_cleaned[:500]]\n",
        "\n",
        "cleaned_jd = [[word for word in text if word not in stopWords] for text in filtered_only]\n",
        "\n"
      ],
      "metadata": {
        "id": "Vm0YCC1T0V29"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gensim dictionary from the processced job description\n",
        "dictionary = corpora.Dictionary(cleaned_jd)\n",
        "\n",
        "# Remove the extreme vocab based on term frequency\n",
        "dictionary.filter_extremes(no_below=1,no_above=0.9)\n",
        "\n",
        "#convert dictionary to a bag of words corpus\n",
        "corpus = [dictionary.doc2bow(jd) for jd in cleaned_jd]"
      ],
      "metadata": {
        "id": "Nc4mDiD4045Z"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WHg2riX3hw5",
        "outputId": "00de7031-6698-43ae-efbc-69f11008c11c"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary(1779 unique tokens: ['BELOW', 'SEE', 'accessment', 'administration', 'incident']...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create LDA model with 20 different clusters\n",
        "lda = models.LdaModel(corpus, num_topics=20, \n",
        "                            id2word=dictionary, \n",
        "                            update_every=5, \n",
        "                            chunksize=10000, \n",
        "                            passes=100)\n",
        "lda.show_topics()"
      ],
      "metadata": {
        "id": "n68yozf52aX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
        "topics_matrix = np.array(topics_matrix)\n",
        "\n",
        "topic_words = topics_matrix[:,1]\n",
        "for i in topic_words:\n",
        "    print([str(word[0]) for word in i])\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FULSgZWw2s7d",
        "outputId": "4911ac14-bec4-444a-e954-fca09c84bc89"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['-', 'Software', 'data', 'C++', 'MS', 'Technical', 'Experience', 'customers', 'Support', 'software', 'Engineer', 'Windows', 'Pre-Sales', 'engineering', 'Amazon', 'EE', 'Product', 'Customer', 'Linux', 'experience']\n",
            "\n",
            "['Design', 'data', 'business', 'development', 'FBA', 'analysis', 'Spring', 'product', 'management', 'Hibernate', 'The', 'Amazon', 'recommendations', 'develop', 'including', 'global', 'System', 'project', 'pricing', 'projects']\n",
            "\n",
            "['delivering', 'including', 'PGS', 'specializes', 'workforce', 'secure', 'reliable', 'rapidly', 'solutions', 'Federal', 'prime', 'marketplace', 'implemented', 'contractors', 'agencies', 'Wi', 'TAD', 'Government', 'MongoDB', 'SCRUM']\n",
            "\n",
            "['Business', 'Analyst', 'Requirements', 'services', 'Supervision', 'Robotic', 'Office', 'MS', 'Data', 'J2EE', 'System', 'Application', 'IT', 'Marketing', 'NoSQL', 'wireless', 'CRM', 'Voip', 'Engineer', 'restful']\n",
            "\n",
            "['Ruby', 'Rails', 'JavaScript', 'web', 'On', 'applications', '-', 'Electronics', 'Engineer', 'services', 'Ember', 'React', 'AWS', 'Services', 'Web', 'Development', 'Product', 'Rest', 'ActiveRecord', 'TDD']\n",
            "\n",
            "['OR', 'AND', 'Office', 'MS', 'year', 'MVC', 'C', 'SalesForce', 'JAVA', 'Delivery', '365', 'WEB', 'Word', 'SERVICES', 'JUNIT', '4', 'TESTING', 'AWS', 'SDLC', 'Scheduling']\n",
            "\n",
            "['Full', 'Time', 'C++', 'Linux', 'development', 'Server', 'NOT', 'essential', 'Windows', 'app', 'embedded', 'Jenkins', 'RESTful', 'experience', 'JavaScript', '-', 'web', 'environment', 'Manager', 'management']\n",
            "\n",
            "['SQL', 'JavaScript', '-', 'Cloud', 'Data', 'C', 'Server', 'MySQL', 'PHP', 'Oracle', 'Sales', 'User', 'AML', 'Salesforce', 'Support', 'HR', 'Design', 'OOP', 'Healthcare', 'HTML']\n",
            "\n",
            "['Experience', 'experience', 'required', 'years', 'available', 'Telecommuting', 'Travel', 'equivalent', 'VMware', 'Information', 'work', 'preferred', 'Engineering', 'storage', 'skills', 'Skills', 'Expert', 'Computer', '5+', 'Video']\n",
            "\n",
            "['Project', 'Management', 'Development', 'Manager', 'Consulting', 'HTTP', 'Excel', 'Testing', 'Analysis', 'HTML', 'SAP', 'Analyst', 'Lifecycle', 'PowerPoint', 'Research', 'Sales', 'Agile', 'Recruiter', 'QA', 'CSS']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ]
    }
  ]
}